\documentclass{article}

%Linux command prompt showing package
\usepackage{listings,menukeys}

%Page format
\usepackage{pdfpages,fancyhdr}
\usepackage[margin=1in]{geometry}

%Math packages and custom commands
\usepackage{algpseudocode,amsmath,framed,tikz,sans,mathtools,amsthm,enumitem,amssymb,dsfont,tabularray,fancyvrb}
\usepackage[short]{optidef}
\usepackage[hidelinks]{hyperref}
\usepackage[table,xcdraw]{xcolor}
%\usepackage[labelformat=empty]{caption}
\usepackage[utf8]{inputenc}
\usetikzlibrary{arrows.meta}
\usepackage[TABcline]{tabstackengine}
\TABstackMath


\usepackage[table]{xcolor}
\usepackage{collcell}
\usepackage{hhline}
\usepackage{pgf}
\usepackage{multirow}

\def\colorModel{hsb}
\newcommand\ColCell[1]{
	\pgfmathparse{#1<50?1:0}  %Threshold for changing the font color into the cells
	\ifnum\pgfmathresult=0\relax\color{white}\fi
	\pgfmathsetmacro\compA{0}      %Component R or H
	\pgfmathsetmacro\compB{#1/100} %Component G or S
	\pgfmathsetmacro\compC{1}      %Component B or B
	\edef\x{\noexpand\centering\noexpand\cellcolor[\colorModel]{\compA,\compB,\compC}}\x #1
} 
\newcolumntype{E}{>{\collectcell\ColCell}m{0.4cm}<{\endcollectcell}}  %Cell width
\newcommand*\rot{\rotatebox{90}}

%Course Info
\newcommand{\coursenumber}{ECE421}
\newcommand{\coursename}{\coursenumber: Introduction to Machine Learning}
\newcommand{\courseyear}{2024}
\newcommand{\coursesemester}{Fall}
\newcommand{\coursefullname}{\coursename~---~\coursesemester~\courseyear}
%Handout Info
\newcommand{\haname}{}
\newcommand{\hatypeandnun}{Worksheet 4: K-means Clustering and Gaussian Mixture Model}


\DeclareMathOperator{\R}{\mathbb{R}}
\newcommand{\probP}{\mathds{P}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{\text{Var}}
\DeclareMathOperator{\Cov}{\text{Cov}}
\renewcommand{\arraystretch}{1.3}
\newcommand{\norm}[2][2]{\| #2\|_{#1}}

\newcommand{\notp}{\neg p}
\newcommand{\notq}{\neg q}
\newcommand{\notr}{\neg r}
\newcommand{\notP}{\neg P}
\newcommand{\notQ}{\neg Q}

\newcommand{\setD}{\mathcal{D}}
\newcommand{\setR}{\mathbb{R}}
\newcommand{\setRd}{\mathbb{R}^d}
\newcommand{\setRdone}{\mathbb{R}^{d+1}}
\renewcommand{\vec}[1]{\underline{#1}}
\newcommand{\vecw}{\vec{w}}
\newcommand{\vecx}{\vec{x}}
\newcommand{\vecy}{\vec{y}}
\newcommand{\vecxn}{\vec{x}_n}
\newcommand{\setxnyn}{\left\{(\vecxn, y_n)\right\}_{n=1}^{N}}


\newcommand{\con}{\wedge}
\newcommand{\dis}{\vee}
\newcommand{\false}{\text{F}}
\newcommand{\terue}{\text{T}}

% verbatim
\DefineShortVerb{\|}
\SaveVerb{py}|Python|
\SaveVerb{num}|NumPy|
\SaveVerb{sci}|scikit-learn|
\SaveVerb{tp1}|test_Part1()|
\SaveVerb{tp2}|test_Part2()|

\definecolor{shadecolor}{gray}{0.9}

\theoremstyle{definition}
\newtheorem*{answer}{Answer}
\newcommand{\note}[1]{\noindent{[\textbf{NOTE:} #1]}}
\newcommand{\hint}[1]{\noindent{[\textbf{HINT:} #1]}}
\newcommand{\recall}[1]{\noindent{[\textbf{RECALL:} #1]}}
\newcommand{\remark}[1]{\noindent{[\textbf{REMARK:} #1]}}
\newcommand{\expect}[1]{\noindent{[\textbf{What we expect:} #1]}}
\newcommand{\mysolution}[1]{\noindent{\begin{shaded}\textbf{Your Solution:}\ #1 \end{shaded}}}

\newlist{question}{enumerate}{3}
\setlist[question, 1]{label=\Large \textbf{Q\arabic{questioni}}, leftmargin=\parindent, rightmargin=10pt}
\setlist[question, 2]{label=\large \textbf{\arabic{questioni}.\alph{questionii}}, leftmargin=15pt, rightmargin=15pt}
\setlist[question, 3]{label=\textbf{\arabic{questioni}.\alph{questionii}.\kern1.5pt\roman{questioniii}},	leftmargin=30pt, rightmargin=30pt}

\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
	\hspace{-2.5pt}}
\newcommand{\wontfix}{\rlap{$\square$}{\large\hspace{1pt}\xmark}}


\title{\vspace{-2.5cm}\textbf{\coursefullname}\\\hatypeandnun\\\haname}
\date{}

%\chead{}
\rhead{\haname}
\lfoot{}
\cfoot{\coursenumber~\coursesemester~\courseyear~---~\hatypeandnun}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\pagestyle{fancy}
\setlength{\parindent}{0pt}

% NOTE(joe): This environment is credit @pnpo (https://tex.stackexchange.com/a/218450)
\lstnewenvironment{algorithm}[1][] %defines the algorithm listing environment
{   
	\lstset{ %this is the stype
		mathescape=true,
		frame=tB,
		numbers=left, 
		numberstyle=\tiny,
		basicstyle=\rmfamily\scriptsize, 
		keywordstyle=\color{black}\bfseries,
		keywords={,procedure, div, mod, for, to, input, output, return, datatype, function, in, if, else, foreach, while, begin, end, } %add the keywords you want, or load a language as Rubens explains in his comment above.
		numbers=left,
		xleftmargin=.04\textwidth,
		#1 % this is to add specific settings to an usage of this environment (for instnce, the caption and referable label)
	}
}
{}

\begin{document}

\maketitle
\vspace*{-2cm}
\begin{question}[start=1]
	\item \textbf{(Lack of Optimality of $K$-Means)} Consider a $K$-Means clustering problem instance with $K = 2$ and a dataset of $4$ points in $\mathbb{R}$ as follows: $x_1 = -12$, $x_2 = -3$, $x_3 = 3$, and $x_4 = 12$. Initialize $K$-Means with the centroids $\mu_1 = -3$ and $\mu_2 = 12$. Demonstrate that in the problem instance above, $K$-Means converges to a solution that is not globally optimal.
	\begin{answer}
		The centroids are initially set to $\mu_1[0] = -3$ and $\mu_2[0] = 12$. It can be easily verified that with these centroids, the K-means algorithm assigns $\mathcal{B}_1[1] = \{x_1, x_2, x_3\}$ and $\mathcal{B}_2[1] = \{x_4\}$. In the next step, the centroids will be updated to $\mu_1[1] = \frac{x_1 + x_2 + x_3}{3} = -4$ and $\mu_2[1] = x_4=12$. It is easy to check that the set memberships will remain unchanged in the next step. Thus, K-Means algorithm converged to the following set membership and centroids: $$\hat{\mathcal{B}}_1 = \{x_1, x_2, x_3\}, \hat{\mathcal{B}}_2[1] = \{x_4\}, \hat{\mu}_1 =-4, \hat{\mu}_2 = 12$$. The error at the converged solution is given by $E = (x_1 - \hat{\mu}_1)^2 + (x_2 - \hat{\mu}_1)^2 + (x_3 - \hat{\mu}_1)^2 + (x_4 - \hat{\mu}_2)^2 = 8^2 + 1^2 + 7^2 + 0 = 114$.\\
		
		To demonstrate that this is not a globally optimal solution, consider the following set memberships and centroids: $$\mathcal{B}^\star_1 = \{x_1, x_2\}, \mathcal{B}^\star_2 = \{x_3, x_4\}, \mu^\star_1 = -7.5, \mu^\star_2 = 7.5.$$ With such set memberships and centroids, the clustering error would be $E^\star = 4(4.5)^2 = 81$, which is lower than $E$.
	\end{answer}
	
	\item \textbf{(K-means algorithm: Problem 6 - Final Exam 2018)} Consider the $K$-means algorithm. Let $K = 2$ and let $\mathcal{D}$ be a dataset consisting of four data points with $\mathcal{D} = \{0, 0.5, 0.5 + \Delta, 1.5 + \Delta\}$, where $\Delta \geq 0$ is a problem parameter. All data points lie on the real line.
	\begin{question}
		\item \label{parta}Let $\Delta = 0.5$ and initialize $K$-means by initializing the two cluster centers at $\mu_1 = 1$ and $\mu_2 = 2$. Run $K$-means till convergence. For each iteration $l$ until convergence, describe your set membership $\{\mathcal{B}_1[l], \mathcal{B}_2[l]\}$ and cluster centers $\{\mu_1[l], \mu_2[l]\}$. Make sure you identify the final values of the cluster centers and set membership at convergence.
		\begin{answer}
			Observe that  $\mathcal D = \{0, 0.5, 1, 2\}$ and the centriods are initialized as $\mu_1[0] = 1, \mu_2[0] = 2$. Thus, we would have the following iterations:
			\begin{itemize}
				\item \textbf{Iteration $1$:} $\mathcal{B}_{1}[1] = \{0, 0.5, 1\}$ and $\mathcal{B}_{2}[1] = \{2\}$; $\mu_{1}[1] = 0.5$ and $\mu_{2}[1] = 2$.
				\item \textbf{Iteration $2$:} $\mathcal{B}_{1}[2] = \{0, 0.5, 1\}$ and $\mathcal{B}_{2}[2] = \{2\}$; $\mu_{1}[2] = 0.5$ and $\mu_{2}[2] = 2$. The cluster assignments and cluster centroids do not change. Done!
			\end{itemize}
		\end{answer}
		\item For this part, find a condition that $\Delta$ must satisfy, such that $\Delta$ has a small positive value, and $K$-means (initialized in the same manner as in~\ref{parta}, \textit{i.e.}, $\mu_1 = 1$ and $\mu_2 = 2$) converges to a different solution from that obtained in~\ref{parta}. In your solution, describe:
		\begin{question}
			\item What is this condition on $\Delta$ and explain your reasoning/derivation.
			\begin{answer}
				It is easy to check that for any $\Delta > 1$, in the first iteration of K-Means algorithm, we would have  $\mathcal{B}_{1}[1] = \{0, 0.5\}$ and $\mathcal{B}_{2}[1] = \{0.5+\Delta, 1.5+\Delta\}$; $\mu_{1}[1] = 0.25$ and $\mu_{2}[1] = 1+\Delta$. And the set memberships and centroid would remain unchanged in the next iteration. Hence, we would converge to a different solution.\\
				
				One can easily verify that if $0< \Delta < 1$, we would always converge to $\mathcal{B}_{1} = \{0, 0.5\}$ and $\mathcal{B}_{2} = \{0.5+\Delta, 1.5+\Delta\}$. Let $x_3 = 0.5 + \Delta$. Observe that for any $0< \Delta < 1$, $\mathcal{B}_{1}[1] = \{0, 0.5, 0.5 + \Delta\}, \mathcal{B}_{2}[1] = \{1.5+\Delta\}$, and $\mu_{1}[1] = \frac{1+\Delta}{3}$ and $\mu_{2}[1] = 1.5+\Delta$. It is easy to show that $\mid\!x_3 - \mu_{1}[1]\!\mid < \mid\!\mu_{2}[1] - x_3\!\mid$, when $0 < \Delta < 1$, and we will converge to the same solution as in~\ref{parta}.
				
%				However, we need to see if it is possible to converge to a different solution with any smaller $\Delta$. With any $\Delta < 1$, in the first iteration of K-Means algorithm, we would have  $\mathcal{B}_{1}[1] = \{0, 0.5, 0.5+\Delta\}, \mathcal{B}_{2}[1] = \{1.5+\Delta\}$ and $\mu_{1}[1] = \frac{1+\Delta}{3}$ and $\mu_{2}[1] = 1.5+\Delta$. Is it possible that in the next iteration, we have the point $0.5+\Delta$ being closer to $\mu_{2}[1]$ than $\mu_{1}[1]$? Thus, we want
%				\begin{align*}
%					\mid\!0.5+\Delta - \mu_{1}[1]\!\mid > \mid\!\mu_{2}[1] - (0.5+\Delta)\!\mid &\Leftrightarrow 0.5+\Delta - \frac{1+\Delta}{3} > 1.5+\Delta -  (0.5+\Delta)\\
%					&\Leftrightarrow \frac{2\Delta}{3} > \frac{5}{6}\\
%					&\Leftrightarrow \Delta > \frac{5}{4}
%				\end{align*}
			\end{answer}
			\item As in~\ref{parta}, run the cluster algorithm, describe the values of cluster centers and set membership for each iteration until convergence.
			\begin{answer}
				With $\Delta > 1$,
				\begin{itemize}
					\item \textbf{Iteration 1:} $\mathcal{B}_{1}[1] = \{0, 0.5\}$ and $\mathcal{B}_{2}[1] = \{0.5+\Delta, 1.5+\Delta\}$; $\mu_{1}[1] = 0.25$ and $\mu_{2}[1] = 1+\Delta$.
					\item \textbf{Iteration 2:} $\mathcal{B}_{1}[2] = \{0, 0.5\}$ and $\mathcal{B}_{2}[2] = \{0.5+\Delta, 1.5+\Delta\}$; $\mu_{1}[2] = 0.25$ and $\mu_{2}[2] = 1+\Delta$. Converged!
				\end{itemize}
			\end{answer}
		\end{question}
	\end{question}
	
	\item \textbf{(Gaussian Mixture Model: Problem 5 - Final Exam 2018)} Consider an already-trained Gaussian Mixture Model (GMM) that is trained to fit data on student performance in a class. The GMM uses two components ($K = 2$) as the class consists of two categories of students: undergraduate students (category 1) and graduate students (category 2). The learned parameters of the GMM are as follows.
	\begin{itemize}
		\item The weights of the two categories (\textit{i.e.}, the responsibilities) are $\pi_1 = \frac{2}{3}$ (undergraduate) and $\pi_2 = \frac{1}{3}$ (graduate).
		\item The distribution that fits scores in category 1 is $\mathcal{N}\left(x; 70, 10^2\right)$.
		\item The distribution that fits scores in category 2 is $\mathcal{N}\left(x; 80, 5^2\right)$.
	\end{itemize}
	\begin{question}
		\item According to the GMM, what is the probability that an arbitrarily selected student scores greater than $80$\%? That is, compute $\probP[X \geq 80]$, where $X$ denotes the score of the student. In your computation, use the approximation that for zero-mean $\sigma^2$-variance random variable $Z$, $\probP[\mid\! Z\!\mid \leq \sigma] = \frac{2}{3}$.
		\begin{answer}
			Note that
			\begin{align*}
				p(x) = \pi_1 \mathcal{N}\left(x \mid 70, 10^2\right) + \pi_2 \mathcal{N}\left(x \mid 80, 5^2\right).
			\end{align*}
			Thus,
			\begin{align*}
				\probP[X \geq 80] &= \pi_1 \probP_{X \sim \mathcal{N}\left(70, 10^2\right)} [X \geq 80] + \pi_2 \probP_{X \sim \mathcal{N}\left(80, 5^2\right)}[X \geq 80]\\
				&=  \frac{2}{3} \cdot \frac{1}{2} \left(1 - \probP_{X \sim \mathcal{N}\left(70, 10^2\right)} [\mid\!X\!\mid < 80]\right) + \frac{1}{3} \cdot \frac{1}{2}\\
				&= \frac{1}{3} \left(1 - \probP_{X \sim \mathcal{N}\left(0, 10^2\right)} [\mid\!X\!\mid < 10]\right) + \frac{1}{6}\\
				&\approx \frac{1}{3} \cdot \frac{1}{3} + \frac{1}{6} = \frac{5}{18}.
			\end{align*}
		\end{answer}
		\item If a particular student has a score greater than $80$, what is the probability that the student is from category $1$? That is, compute $\probP[\text{class} = 1 \mid X \geq 80]$. (Use the same approximation as in the previous part.)
		\begin{answer}
			Applying the Bayes' rule, \textit{i.e.}, $\probP[A\mid B] = \frac{\probP[B\mid A]\probP[A]}{\probP[B]}$, we have
			\begin{align*}
				\probP[\text{class} = 1 \mid X \geq 80] & = \frac{\probP[(X \geq 80) \mid \text{class} = 1] \probP[\text{class} = 1]}{\probP[(X \geq 80)]} \\
				& = \probP_{X \sim \mathcal{N}\left(70, 10^2\right)}[X \geq 80] \times \pi_1 \times \frac{18}{5} \\
				& = \frac{2}{5}.
			\end{align*}
		\end{answer}
	\end{question}
\end{question}
\end{document}