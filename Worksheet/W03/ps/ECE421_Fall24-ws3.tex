\documentclass{article}

%Linux command prompt showing package
\usepackage{listings,menukeys}

%Page format
\usepackage{pdfpages,fancyhdr}
\usepackage[margin=1in]{geometry}

%Math packages and custom commands
\usepackage{algpseudocode,amsmath,framed,tikz,sans,mathtools,amsthm,enumitem,amssymb,dsfont,tabularray,fancyvrb}
\usepackage[short]{optidef}
\usepackage[hidelinks]{hyperref}
\usepackage[table,xcdraw]{xcolor}
%\usepackage[labelformat=empty]{caption}
\usepackage[utf8]{inputenc}
\usetikzlibrary{arrows.meta}
\usepackage[TABcline]{tabstackengine}
\TABstackMath


\usepackage[table]{xcolor}
\usepackage{collcell}
\usepackage{hhline}
\usepackage{pgf}
\usepackage{multirow}

\def\colorModel{hsb}
\newcommand\ColCell[1]{
	\pgfmathparse{#1<50?1:0}  %Threshold for changing the font color into the cells
	\ifnum\pgfmathresult=0\relax\color{white}\fi
	\pgfmathsetmacro\compA{0}      %Component R or H
	\pgfmathsetmacro\compB{#1/100} %Component G or S
	\pgfmathsetmacro\compC{1}      %Component B or B
	\edef\x{\noexpand\centering\noexpand\cellcolor[\colorModel]{\compA,\compB,\compC}}\x #1
} 
\newcolumntype{E}{>{\collectcell\ColCell}m{0.4cm}<{\endcollectcell}}  %Cell width
\newcommand*\rot{\rotatebox{90}}

%Course Info
\newcommand{\coursenumber}{ECE421}
\newcommand{\coursename}{\coursenumber: Introduction to Machine Learning}
\newcommand{\courseyear}{2024}
\newcommand{\coursesemester}{Fall}
\newcommand{\coursefullname}{\coursename~---~\coursesemester~\courseyear}
%Handout Info
\newcommand{\haname}{}
\newcommand{\hatypeandnun}{Worksheet 3: Gradient Descent, Multi-class Logistic Regression, Regularized Regression Model}


\DeclareMathOperator{\R}{\mathbb{R}}
\newcommand{\probP}{\mathds{P}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{\text{Var}}
\DeclareMathOperator{\Cov}{\text{Cov}}
\renewcommand{\arraystretch}{1.3}
\newcommand{\norm}[2][2]{\| #2\|_{#1}}

\newcommand{\notp}{\neg p}
\newcommand{\notq}{\neg q}
\newcommand{\notr}{\neg r}
\newcommand{\notP}{\neg P}
\newcommand{\notQ}{\neg Q}

\newcommand{\setD}{\mathcal{D}}
\newcommand{\setR}{\mathbb{R}}
\newcommand{\setRd}{\mathbb{R}^d}
\newcommand{\setRdone}{\mathbb{R}^{d+1}}
\renewcommand{\vec}[1]{\underline{#1}}
\newcommand{\vecw}{\vec{w}}
\newcommand{\vecx}{\vec{x}}
\newcommand{\vecy}{\vec{y}}
\newcommand{\vecxn}{\vec{x}_n}
\newcommand{\setxnyn}{\left\{(\vecxn, y_n)\right\}_{n=1}^{N}}


\newcommand{\con}{\wedge}
\newcommand{\dis}{\vee}
\newcommand{\false}{\text{F}}
\newcommand{\terue}{\text{T}}

% verbatim
\DefineShortVerb{\|}
\SaveVerb{py}|Python|
\SaveVerb{num}|NumPy|
\SaveVerb{sci}|scikit-learn|
\SaveVerb{tp1}|test_Part1()|
\SaveVerb{tp2}|test_Part2()|

\definecolor{shadecolor}{gray}{0.9}

\theoremstyle{definition}
\newtheorem*{answer}{Answer}
\newcommand{\note}[1]{\noindent{[\textbf{NOTE:} #1]}}
\newcommand{\hint}[1]{\noindent{[\textbf{HINT:} #1]}}
\newcommand{\recall}[1]{\noindent{[\textbf{RECALL:} #1]}}
\newcommand{\remark}[1]{\noindent{[\textbf{REMARK:} #1]}}
\newcommand{\expect}[1]{\noindent{[\textbf{What we expect:} #1]}}
\newcommand{\mysolution}[1]{\noindent{\begin{shaded}\textbf{Your Solution:}\ #1 \end{shaded}}}

\newlist{question}{enumerate}{3}
\setlist[question, 1]{label=\Large \textbf{Q\arabic{questioni}}, leftmargin=\parindent, rightmargin=10pt}
\setlist[question, 2]{label=\large \textbf{\arabic{questioni}.\alph{questionii}}, leftmargin=15pt, rightmargin=15pt}
\setlist[question, 3]{label=\textbf{\arabic{questioni}.\alph{questionii}.\kern1.5pt\roman{questioniii}},	leftmargin=30pt, rightmargin=30pt}

\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
	\hspace{-2.5pt}}
\newcommand{\wontfix}{\rlap{$\square$}{\large\hspace{1pt}\xmark}}


\title{\vspace{-2.5cm}\textbf{\coursefullname}\\\hatypeandnun\\\haname}
\date{}

%\chead{}
\rhead{\haname}
\lfoot{}
\cfoot{\coursenumber~\coursesemester~\courseyear~---~\hatypeandnun}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\pagestyle{fancy}
\setlength{\parindent}{0pt}

% NOTE(joe): This environment is credit @pnpo (https://tex.stackexchange.com/a/218450)
\lstnewenvironment{algorithm}[1][] %defines the algorithm listing environment
{   
	\lstset{ %this is the stype
		mathescape=true,
		frame=tB,
		numbers=left, 
		numberstyle=\tiny,
		basicstyle=\rmfamily\scriptsize, 
		keywordstyle=\color{black}\bfseries,
		keywords={,procedure, div, mod, for, to, input, output, return, datatype, function, in, if, else, foreach, while, begin, end, } %add the keywords you want, or load a language as Rubens explains in his comment above.
		numbers=left,
		xleftmargin=.04\textwidth,
		#1 % this is to add specific settings to an usage of this environment (for instnce, the caption and referable label)
	}
}
{}

\begin{document}

\maketitle
\vspace*{-2cm}
\begin{question}[start=0]
	\item \label{q0}\textbf{(Matrix Decomposition Primer)} The singular value decomposition is frequently used as part of unsupervised learning algorithms. In particular, the SVD forms the basis of an algorithm called principal components analysis (PCA), which reduces data dimensions such that the reduced data still contains the maximum amount of variability given the number of reduced dimensions. The reduction axes picked by PCA directly correspond to the singular vectors identified by the singular value decomposition. In the following questions, we review matrix decomposition. \\\note{Your TAs won't review~\ref{q0} during the tutorial session. Please review your linear algebra notes/references if you are struggling with the following questions.}
	\begin{question}
		\item \textbf{(Eigen-based Decomposition)} For a square diagonalizable matrix $A \in \mathbb{R}^{n \times n}$ the eigendecomposition is given by $A = Q \Lambda Q^{-1}$, where $Q \in \mathbb{R}^{n \times n}$ is a square matrix that contains the eigenvectors $\underline{q}_i \in \mathbb{R}^{n}$ as columns, and $\Lambda$ is a diagonal matrix\footnote{A matrix $M$ is called diagonal if all the entries outside of its main diagonal are $0$. The main diagonal is the top-left to bottom-right diagonal.} whose entries correspond to the eigenvalues of the respective eigenvectors from $Q$. In particular $\Lambda_{ii} = \lambda_i$ is the eigenvalue associated with eigenvector $\underline{q}_i$.
		\footnote{A non-zero vector $\underline{v} \in \mathbb{R}^n$ is called an eigenvector of a square diagonalizable matrix $A \in \mathbb{R}^{n \times n}$ if, for a scalar $\lambda \in \mathbb{R}$ it satisfies $A\underline{v} = \lambda \underline{v}$. Intuitively speaking, an eigenvector $\underline{v}$ is a vector which, under the transformation applied by $A$, is only scaled in its magnitude. In particular, such vectors $\underline{v}$ stay on their own span and are only elongated or shrinked. The degree of change in magnitude inflicted by $A$ can be summarized in a single scalar $\lambda$, which is the eigenvalue corresponding to $\underline{v}$. The set of all eigenvalue-eigenvector combinations can be computed by solving for the characteristic polynomial yielded by $det(A - \lambda I) = 0$.}\\

		Compute the characteristic polynomial of $A = \begin{bmatrix}
			4 & 0 & -2 \\
			1 & 3 & -2 \\
			1 & 2 & -1
		\end{bmatrix}$ and determine its eigenvalues and eigenvectors. Then, find the eigendecomposition of $A$.
		\item \textbf{(Singular Value Decomposition)} The Singular Value Decomposition (SVD) generalizes the concepts from the eigendecomposition to general matrices $A \in \mathbb{R}^{m \times n}$ by decomposing it as $A = U \Sigma V^\top$, where, $U \in \mathbb{R}^{m \times m}$ is an orthogonal matrix\footnote{A square matrix $M$ is called orthogonal if $MM^\top = M^\top M = I$.}, $\Sigma \in \mathbb{R}^{m \times n}$ is a diagonal matrix, and $V \in \mathbb{R}^{n \times n}$ is an orthogonal matrix.\\
		Derive the singular value decomposition of $A = \begin{bmatrix}
			3 & 2 & 2\\
			2 & 3 & -2
		\end{bmatrix}$ with following steps.\footnote{A nice visualization of SVD can be found here: https://youtu.be/vSczTbgc8Rc?si=iIYcflhGdcdYV2bv}
		\begin{question}
			\item Compute $A^\top A$, yielding a square symmetric matrix.
			\item Find the eigenvalues for $A^\top A$.
			\item Find the eigenvectors for $A^\top A$.
			\item Normalize the eigenvectors of $A^\top A$ to get $V$.
			\item Find $U$ using the normalized eigenvectors of $V$ where $\underline{u}_i = \frac{1}{\sigma_i} A \underline{v}_i$.
		\end{question}
	\end{question}
	\item \textbf{(Regularized Linear Regression)} Many machine learning algorithms use $\ell_p$ norms to either
	\begin{itemize}
		\item measure the distance between points in a high-dimensional data space (\textit{e.g.}, $k$-nearest neighbor classification); or
		\item bound the magnitude of a vector to a specific value (\textit{e.g.}, regularization in linear regression).
	\end{itemize}
	In this question, you will train a regularized linear regression model with an $\ell_2$ regularization penalty.\\
	
	You are given a dataset $\mathcal{D} = \{(\underline{x}_n, y_n)\}_{n=1}^N$, where $\underline{x}_n \in \mathbb{R}^{d+1}, d \geq 1$, and $y_n \in \{+1,-1\}$. We would like to train a regularized linear regression model, where the mean squared loss is augmented with an $\ell_2$ regularization penalty $\|\underline{w}\|_2^2$ on the weight parameter $\underline{w}  \in \mathbb{R}^{d+1}$, \textit{i.e.},
	\begin{equation*}
		E_{\text{in}}(\underline{w}) = \frac{1}{2N} \sum_{n=1}^N\left(\underline{w}^\top \underline{x}_n - y_n\right)^2 + \frac{\lambda}{2}\|\underline{w}\|_2^2 = \frac{1}{2N} \|X\underline{w} - \underline{y}\|_2^2 + \frac{\lambda}{2}\|\underline{w}\|_2^2.
	\end{equation*} 
		where $\lambda > 0$ is a hyperparameter controlling the penalty, and the data matrix $X$ and target vector $\vec{y}$ are defined as $X = \begin{bmatrix}
			\vecx_{1}^\top \\
			\vdots \\
			\vecx_{N}^\top
		\end{bmatrix} \in \setR^{N \times (d+1)}$, and $\vec{y} = \begin{bmatrix}
		y_1 \\
		\vdots \\
		y_N
		\end{bmatrix}\in \setR^{N}$.
		\begin{question}
			\item \label{q}Prove that all eigenvalues of $X^\top X$ are non-negative.\\\hint{An $n \times n$ real matrix $M$ is is said to be \textbf{positive-semidefinite} if $\underline{z}^\top M \underline{z} \geq 0$, for any non-zero $\underline{z} \in \mathbb{R}^n$. One can show that $M \in \mathbb{R}^{n \times n}$ is positive-semidefinite \textbf{if and only if} all of its eigenvalues are non-negative. So, to prove that all eigenvalues of $X^\top X$ are non-negative, it suffices to prove that $\underline{z}^\top X^\top X \underline{z} \geq 0$ for any non-zero vector $\underline{z}$.}
			\item Demonstrate that matrix $X^\top X + \lambda N  I$, where $I$ is the identity matrix, is invertible by proving that none of its eigenvalues are zero.\\\hint{Assume $\underline{v}$ is an eigenvector of $X^\top X$ with corresponding eigenvalue $\mu$. In~\ref{q}, you proved that $\mu \geq 0$. Now, show that $\underline{v}$ is also an eigenvector of $X^\top X + \lambda N  I$ and find its corresponding eigenvalue.}
			\item Using the invertibility of matrix $X^\top X + \lambda N  I$, find an analytical solution for $\underline{w}^\star = \underset{\underline{w}}{\arg \min} E_{\text{in}}(\underline{w})$. Congratulations! You have trained a regularized linear regression model with an $\ell_2$ regularization penalty.
		\end{question}
		\item \textbf{(Hinge Loss and Stochastic Gradient Descent)} Consider the same \textit{Rotten Tomatoes} reviews in the previous worksheet:
		\begin{quote}
			\textbf{Panos Kotzathanasis} (Asian Movie Plus): "Perfect Blue" is an artistic and technical masterpiece; however, what is of outmost importance is the fact that Satoshi Kon never deteriorate from the high standards he set here, in the first project that was entirely his own.\\
			
			\textbf{Derek Smith} (Cinematic Reflections): [An] nime thriller [that] often plays as an examination of identity and celebrity, but ultimately gets so lost in its own complex structure that it doesn't end up saying much at all.
		\end{quote}
		
		Rotten Tomatoes has classified these reviews as ``positive'' and ``negative,'' respectively.\\
		
		In this assignment, you will create a simple text classification system that can perform this task automatically. similar to the previous worksheet, we'll warm up with the following set of four mini-reviews, each labeled positive $(+1)$ or negative $(-1)$:
		\begin{enumerate}
			\item $\underline{x}_1:$ not good; label: $(-1)$
			\item $\underline{x}_2:$ pretty bad; label: $(-1)$
			\item $\underline{x}_3:$ good plot; label: $(+1)$
			\item $\underline{x}_4:$ pretty scenery; label: $(+1)$
		\end{enumerate}
		
		Each review $\underline{x}$ is mapped onto a feature vector $\phi(\underline{x})$, which maps each word to the number of occurrences of that word in the review and adds a $1$ to account for bias. For example, the second review maps to the (sparse) feature vector $\phi(\underline{x}_2)=\{\textbf{extra added 1}:1, \textbf{pretty}:1,\textbf{bad}:1\}$. The hinge loss for a single datapoint is defined as
		\begin{equation*}
			L_{\text{hinge}}(\underline{x}, y,\underline{w})= \max\{0, 1-y\underline{w}^\top\phi(\underline{x})\},
		\end{equation*}
		where $\underline{x}$ is the review text, $y$ is the correct label, $\underline{w}$ is the weight vector.\\

		Suppose we run \textbf{stochastic gradient descent} once for each of the 4 samples in the order given above, updating the weights according to
		\begin{equation*}
			\underline{w} \leftarrow \underline{w} - \epsilon \nabla_{\underline{w}} L_{\text{hinge}}(\underline{x}, y, \underline{w}).
		\end{equation*}
		
		After the updates, what are the bias and the weights of the six words (``pretty'', ``good'', ``bad'', ``plot'', ``not'', ``scenery'') that appear in the above reviews?
		\begin{itemize}
			\item Use $\epsilon=0.1$ as the step size.
			\item Initialize $\underline{w}=[0,0,0,0,0,0,0]$.
			\item The gradient $\nabla_{\underline{w}} L_{\text{hinge}}(\underline{x}, y, \underline{w})=\underline{0}$ when margin is exactly $1$ (\emph{i.e.}, when $y\underline{w}^\top\phi(\underline{x}) = 1$).
		\end{itemize}
		
		Present your answer as a weight vector that contains a numerical value for each of the tokens in the reviews  (``extra added 1'', ``pretty'', ``good'', ``bad'', ``plot'', ``not'', ``scenery'').
	\item \textbf{(Problem 2, Midterm 2019, Multi-class softmax regression model)}
	Suppose we use a multi-class softmax regression model to classify input data vectors $\underline{x} \in \mathbb{R}^{d+1}$ with two possible class labels $y \in \{1, 2\}$. Let $\underline{w}(1)$ and $\underline{w}(2)$ be the weight vectors for class 1 and 2, respectively. For any input $\underline{x}$, we hypothesize that the probability of $\underline{x}$ belonging to class $i \in \{1, 2\}$ is
	\[
	\probP^{\text{SM}}(y = i\mid \underline{x}) = \frac{\exp(\underline{w}(i)^\top \underline{x})}{\exp(\underline{w}(1)^\top \underline{x}) + \exp(\underline{w}(2)^\top \underline{x})}.
	\]
	For a training example $(\underline{x}_n, y_n)$, let the loss function be defined as
	\[
	e^{\text{SM}}_{n}(\underline{w}(1), \underline{w}(2)) = -\log(\probP^{\text{SM}}(y_n\mid \underline{x}_n)) = -\log\left[\frac{\exp(\underline{w}(y_n)^\top \underline{x}_n)}{\exp(\underline{w}(1)^\top\underline{x}_n) + \exp(\underline{w}(2)^\top\underline{x}_n)}\right].
	\]
	\begin{question}
		\item Find the gradients of $e^{\text{SM}}_{n}$ with respect to $\underline{w}(1)$ and $\underline{w}(2)$. (Note that you should always consider two possible values of $y_n$).
		
		\item Suppose instead of the multi-class softmax regression model, we use a binary logistic regression model. For an input $\underline{x} \in \mathbb{R}^{d+1}$, we hypothesize that the probability of $x$ belonging to class 1 is
		\[
		\probP^{\text{LR}}(y = 1\mid \underline{x}) = \frac{\exp({\underline{w}^\top \underline{x}})}{1 + \exp{(\underline{w}^\top \underline{x}})}.
		\]
		Therefore, $\underline{x}$ belongs to class 2 with probability $\probP^{\text{LR}}(y = 2\mid \underline{x}) = 1 - \probP^{\text{LR}}(y = 1\mid \underline{x})$. For an example $(\underline{x}_n, y_n)$ we define the loss function as
		\[
		e^{\text{LR}}_{n}(\underline{w}) = -\log(\probP^{\text{LR}}(y_n\mid \underline{x}_n)).
		\]
		Find a relationship between $\underline{w}(1), \underline{w}(2)$ and $w$, so that we have
		\[
		\probP^{\text{SM}}(y = 1\mid \underline{x}) = \probP^{\text{LR}}(y = 1\mid \underline{x}), \quad \probP^{\text{SM}}(y = 2\mid \underline{x}) = 1 - \probP^{\text{LR}}(y = 1\mid \underline{x}).
		\]
		
		\item Given $(\underline{w}(1), \underline{w}(2))$ and $w$ as described in (b), we apply SGD to separately train the softmax regression model and binary logistic regression model, with constant learning rates $\epsilon^{\text{SM}}$ and $\epsilon^{\text{LR}}$, respectively. For both models, all weights are initialized to zero, and we use the same random seed so that in each iteration of SGD the same random training example is selected. Find a relationship between $\epsilon^{\text{SM}}$ and $\epsilon^{\text{LR}}$, so that $e^{\text{SM}}_{n}(\underline{w}(1), \underline{w}(2))$ and $e^{\text{LR}}_{n}(w)$ are identical in all iterations of SGD.
	\end{question}
\end{question}
\end{document}